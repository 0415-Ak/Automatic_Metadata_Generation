{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11370,
     "status": "ok",
     "timestamp": 1750407225910,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "a-0mbby08HmU",
    "outputId": "769e16a2-0629-42b9-e6d4-af2694d23d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
      "Requirement already satisfied: docx in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
      "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber docx langchain requests python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1750399623681,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "YBg0iwcw7PZu"
   },
   "outputs": [],
   "source": [
    "# Detects file type and extracts text from PDF, DOCX, or TXT (with pdfplumber for tables)\n",
    "import os\n",
    "import pdfplumber\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1750400731689,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "n_MrYeWH8rKo"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_file(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if ext == '.pdf':\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    elif ext == '.docx':\n",
    "        doc = Document(file_path)\n",
    "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif ext == '.txt':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1750400733928,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "dkAkdbEK80Zq",
    "outputId": "81977534-11fa-4fb0-8a67-58349bbc3ab3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    }
   ],
   "source": [
    "text=extract_text_from_file('/content/swiggy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1750400736529,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "Ogt0PB21-Z08",
    "outputId": "949b1017-9987-4bf5-c228-3ad048dd69b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleans and normalizes extracted text using regex + NLP techniques\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1750400739147,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "QsO0pqij-Wtn"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Regex cleaning\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "\n",
    "    # Remove repeating headers/footers\n",
    "    lines = text.splitlines()\n",
    "    line_counts = {}\n",
    "    for line in lines:\n",
    "        line_counts[line] = line_counts.get(line, 0) + 1\n",
    "    lines = [line for line in lines if line_counts[line] < 5]\n",
    "    text = \" \".join(lines)\n",
    "\n",
    "    # NLP preprocessing: stopword removal + lemmatization\n",
    "    doc = nlp(text)\n",
    "    cleaned = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "    return \" \".join(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1750400741173,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "1vGLT5Eg-0Cj"
   },
   "outputs": [],
   "source": [
    "final_text=preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1750422490136,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "6vuVaTgsTzWs"
   },
   "outputs": [],
   "source": [
    "# divide overall data in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1750400742493,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "V4SCqT9c_Ntg"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750411700627,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "vB9e929W_Ted"
   },
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1750400746528,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "4oS3QrgI_d9C",
    "outputId": "6ac7b704-d3f7-48ef-a6de-74c4c45dc55f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Goyal consumer tech platform half decade previously handle business finance vice president senior vice president new role NSoawu rRaeva disin eg xpecte lead initiative delivery partner SAURAV GOYAL SWIGGY SVP delivery previous stint Goyal Ola Consumer Flipkart Tata Communications business finance role lead Business Finance bit energy shift heart Swiggy mission elevate quality life urban consumer offer unparalleled convenience amazing delivery partner backbone reliability reach Goyal share post linkedin connect Swiggy Goyal lead Business Finance replacement appoint month Swiggy appoint Flipkart exec Ankit Jain Senior Vice President Operations previous stint Flipkart Jain responsible platform end end grocery operation large supply chain include design Flipkart quick commerce arm Minutes',\n",
       " 'operation large supply chain include design Flipkart quick commerce arm Minutes supply chain Goyal appointment come competition quick commerce sector heat player fight consumer wallet share dark store location hyper local fleet connect Swiggy edit Affirunisa Kankudti QUICK commerce HYPERLOCAL delivery partner GIG worker']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text_into_chunks(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1750407225959,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "bk0TY34s_d6X"
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an intelligent assistant designed to understand documents and extract structured information from them.\n",
    "\n",
    "Your task is to:\n",
    "1. Extract the following metadata:\n",
    "   - Title (if mentioned)\n",
    "   - Author (if available)\n",
    "   - Date of publication or document creation (if available)\n",
    "   - Keywords or topics covered\n",
    "   - Type of document (choose from: research paper, legal notice, resume, report, book chapter, article, business proposal, letter, others)\n",
    "2. Generate a concise summary of the content (3-5 sentences).\n",
    "\n",
    "Read the content below and return your answer in this JSON format:\n",
    "{{\n",
    "  \"title\": \"\",\n",
    "  \"author\": \"\",\n",
    "  \"date\": \"\",\n",
    "  \"keywords\": [],\n",
    "  \"document_type\": \"\",\n",
    "  \"summary\": \"\"\n",
    "}}\n",
    "\n",
    "Content:\n",
    "\\\"\\\"\\\"{content_chunk}\\\"\\\"\\\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1750401334419,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "_atPso19BQZF"
   },
   "outputs": [],
   "source": [
    "# 5. llm_call.py\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1750403616721,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "oo07SK1-Jy-7"
   },
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_URL\"] = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"xLy5WZAJHVd0AkGgkAcOO6X1psZWo0jY \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750407358129,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "eTovv4uLJ8hW"
   },
   "outputs": [],
   "source": [
    "MISTRAL_API_URL = os.getenv(\"MISTRAL_API_URL\")\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "def call_llm_on_chunk(chunk):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"open-mistral-7b\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(content_chunk=chunk)}\n",
    "        ],\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "    response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"âŒ Error {response.status_code}: {response.text}\")\n",
    "        return \"ERROR\"\n",
    "    return response.json()['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1750403186457,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "0hyWB1W5J8ea"
   },
   "outputs": [],
   "source": [
    "def summarize_document_chunks(chunks):\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        result = call_llm_on_chunk(chunk)\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1750419265920,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "KtByKQaNJ8cf"
   },
   "outputs": [],
   "source": [
    "text = extract_text_from_file('/content/swiggy_instamart.pdf')\n",
    "clean_text = preprocess_text(text)\n",
    "chunks = split_text_into_chunks(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141168,
     "status": "ok",
     "timestamp": 1750415030894,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "MRKiOG_X2331",
    "outputId": "cfd94b3b-46cc-4e27-97b1-832700be2cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (2.0.2)\n",
      "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
      "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m993.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, keybert\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed keybert-0.9.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install keybert sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1750415923038,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "2JMVAJBo3ipG"
   },
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7689,
     "status": "ok",
     "timestamp": 1750419276982,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "49XgQUrrMZsq",
    "outputId": "8a959599-e44e-40e7-a9c1-9ce10cd5ea41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total Chunks: 4\n",
      "\n",
      "--- Generating summary for Chunk 1/4 ---\n",
      "{\n",
      "  \"title\": \"Swiggy Instamart Expands Quick Commerce Services in India\",\n",
      "  \"author\": \"Not specified in the content\",\n",
      "  \"date\": \"Not specified in the content\",\n",
      "  \"keywords\": [\"Swiggy Instamart\", \"quick commerce\", \"expansion\", \"India\", \"festive season\", \"new cities\", \"products\", \"fmcg\", \"local favorites\"],\n",
      "  \"document_type\": \"article\",\n",
      "  \"summary\": \"The text discusses Swiggy Instamart's expansion of its quick commerce services in various cities across India, bringing a wide range of products to customers' doorsteps within minutes. The company aims to make everyday needs more accessible and convenient, with Swiggy Instamart being India's pioneer in quick commerce platforms.\"\n",
      "}\n",
      "\n",
      "--- Generating summary for Chunk 2/4 ---\n",
      "{\n",
      "  \"title\": \"Swiggy Instamart Expands Quick Commerce Service in Tier Cities\",\n",
      "  \"author\": \"Not specified\",\n",
      "  \"date\": \"Not specified\",\n",
      "  \"keywords\": [\"Swiggy Instamart\", \"quick commerce\", \"expansion\", \"tier cities\", \"local brands\", \"fast delivery\"],\n",
      "  \"document_type\": \"article\",\n",
      "  \"summary\": \"The article discusses Swiggy Instamart's expansion of its quick commerce service into tier cities in India, with a focus on providing easy access to everyday essentials and local brands. The service has demonstrated strong uptake in Mangalore, Thrissur, and Bhopal, setting new benchmarks for fast delivery and reaching orders within a single day. Amitesh Jha, CEO of Swiggy Instamart, expresses enthusiasm about the demand and finds the expansion into small towns and cities incredibly encouraging.\"\n",
      "}\n",
      "\n",
      "--- Generating summary for Chunk 3/4 ---\n",
      "{\n",
      "  \"title\": \"Swiggy Instamart Expansion Plan\",\n",
      "  \"author\": \"Not specified\",\n",
      "  \"date\": \"Not specified\",\n",
      "  \"keywords\": [\"Swiggy Instamart\", \"Expansion\", \"Dark Store Network\", \"Quick Commerce\", \"Grocery Delivery\", \"Festive Season\", \"Demand\", \"Assortment\", \"Delivery Fleet\", \"Urban Consumer\", \"Convenience\", \"Food Delivery\"],\n",
      "  \"document_type\": \"Business Proposal\",\n",
      "  \"summary\": \"The document outlines Swiggy Instamart's plan to expand its dark store network to new cities, increase assortment to handle the rise in demand during festive seasons, and provide an expansive selection of products with an impressive minute delivery window. Swiggy Instamart, India's pioneer in quick commerce, aims to bring grocery and daily essentials to consumers' doorsteps using its superior technology and dedicated delivery fleet. The mission is to elevate the quality of life for urban consumers by offering unparalleled convenience with an extensive footprint, collaborating with nearly a lakh of restaurants in the city.\"\n",
      "}\n",
      "\n",
      "--- Generating summary for Chunk 4/4 ---\n",
      "{\n",
      "  \"title\": \"Not specified\",\n",
      "  \"author\": \"Not specified\",\n",
      "  \"date\": \"Not specified\",\n",
      "  \"keywords\": [\"Swiggy\", \"Instamart\", \"quick commerce\", \"grocery\", \"delivery\", \"innovation\", \"Swiggy Dineout\", \"Swiggy Genie\", \"multi-service app\", \"technology\", \"membership program\", \"superior experience\", \"consumer\", \"Swiggy Lite\", \"incognito mode\", \"grocery order\", \"food\"],\n",
      "  \"document_type\": \"Article or News snippet\",\n",
      "  \"summary\": \"The text discusses Swiggy Instamart, a quick commerce platform operated by Swiggy, which delivers groceries and essentials within minutes. The company is committed to innovation and continually incubates and integrates new services like Swiggy Dineout and Swiggy Genie. They leverage cutting-edge technology and offer a membership program with benefits for food and quick commerce dine-and-drop services. Swiggy aims to provide a superior consumer experience and has recently launched Swiggy Lite membership, which offers extra benefits and deletes consumer delivery charges. Additionally, Swiggy has launched an incognito mode for grocery orders and food.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f\"âœ… Total Chunks: {len(chunks)}\")\n",
    "\n",
    "results = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n--- Generating summary for Chunk {i+1}/{len(chunks)} ---\")\n",
    "    summary = call_llm_on_chunk(chunk)\n",
    "    print(summary)\n",
    "    results.append(summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1750419279768,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "8wPx9UbpediO"
   },
   "outputs": [],
   "source": [
    "# After summarizing all chunks:\n",
    "combined_summaries = \"\\n\\n\".join(results)\n",
    "\n",
    "combine_prompt = f\"\"\"\n",
    "You are a smart assistant. Below are multiple partial summaries of a document, generated from different parts.\n",
    "\n",
    "Your task is to combine them into a **single metadata + summary JSON**, like this:\n",
    "{{\n",
    "  \"title\": \"\",\n",
    "  \"author\": \"\",\n",
    "  \"date\": \"\",\n",
    "  \"keywords\": [],\n",
    "  \"document_type\": \"\",\n",
    "  \"summary\": \"\"\n",
    "}}\n",
    "\n",
    "Summaries:\n",
    "\\\"\\\"\\\"{combined_summaries}\\\"\\\"\\\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2uOXJWvGSEK"
   },
   "outputs": [],
   "source": [
    "# def call_llm_merge_summary(prompt):\n",
    "#     headers = {\n",
    "#         \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
    "#         \"Content-Type\": \"application/json\"\n",
    "#     }\n",
    "#     data = {\n",
    "#         \"model\": \"open-mistral-7b\",\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#         \"temperature\": 0.3\n",
    "#     }\n",
    "#     response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
    "#     return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "# final_output = call_llm_merge_summary(combine_prompt)\n",
    "\n",
    "# # ============================\n",
    "# # ğŸ¯ Final Output Handling\n",
    "# # ============================\n",
    "# try:\n",
    "#     parsed = json.loads(final_output)\n",
    "\n",
    "#     # âœ… Improve keywords using KeyBERT\n",
    "#     kw_model = KeyBERT(model=SentenceTransformer('all-MiniLM-L6-v2'))\n",
    "#     kb_keywords = kw_model.extract_keywords(\n",
    "#         clean_text,\n",
    "#         keyphrase_ngram_range=(1, 2),\n",
    "#         stop_words='english',\n",
    "#         top_n=10,\n",
    "#         use_maxsum=True,\n",
    "#         nr_candidates=20\n",
    "#     )\n",
    "#     final_keywords = [kw for kw, score in kb_keywords]\n",
    "#     parsed[\"keywords\"] = final_keywords\n",
    "\n",
    "#     # âœ… Pretty output\n",
    "#     print(\"\\nâœ… Final Metadata:\")\n",
    "#     print(json.dumps(parsed, indent=2))\n",
    "\n",
    "#     print(\"\\nâœ… Final Summary:\")\n",
    "#     print(textwrap.fill(parsed[\"summary\"], width=100))\n",
    "\n",
    "# except json.JSONDecodeError:\n",
    "#     print(\"âš ï¸ Could not parse JSON. Showing raw output:\")\n",
    "#     print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEcy0oVJUQTA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "import re\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGmm5pdBUTgf"
   },
   "outputs": [],
   "source": [
    "def call_llm_merge_summary(prompt):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"open-mistral-7b\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "    response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
    "    return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "final_output = call_llm_merge_summary(combine_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15343,
     "status": "ok",
     "timestamp": 1750419301863,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "93ChQ8bJ561x",
    "outputId": "dc3af16d-d118-402e-d32c-0ff73b084069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final Metadata:\n",
      "{\n",
      "  \"title\": \"Swiggy Instamart Expands Quick Commerce Services in India\",\n",
      "  \"author\": \"Not specified\",\n",
      "  \"date\": \"Not specified\",\n",
      "  \"keywords\": [\n",
      "    \"customer india\",\n",
      "    \"instamart bring\",\n",
      "    \"benefit swiggy\",\n",
      "    \"innovation swiggy\",\n",
      "    \"approach swiggy\",\n",
      "    \"use swiggys\",\n",
      "    \"instamart indias\",\n",
      "    \"food swiggy\",\n",
      "    \"instamarts store\",\n",
      "    \"instamart use\"\n",
      "  ],\n",
      "  \"document_type\": \"Article\",\n",
      "  \"summary\": \"The text discusses Swiggy Instamart's expansion of its quick commerce services, aiming to make everyday needs more accessible and convenient in various cities across India, including tier cities. The company is India's pioneer in quick commerce platforms, continually innovating and integrating new services like Swiggy Dineout and Swiggy Genie. They leverage cutting-edge technology and offer a membership program with benefits for food and quick commerce dine-and-drop services. Swiggy aims to provide a superior consumer experience and has recently launched Swiggy Lite membership, which offers extra benefits and deletes consumer delivery charges. Additionally, Swiggy has launched an incognito mode for grocery orders and food.\"\n",
      "}\n",
      "\n",
      "âœ… Final Summary:\n",
      "The text discusses Swiggy Instamart's expansion of its quick commerce services, aiming to make\n",
      "everyday needs more accessible and convenient in various cities across India, including tier cities.\n",
      "The company is India's pioneer in quick commerce platforms, continually innovating and integrating\n",
      "new services like Swiggy Dineout and Swiggy Genie. They leverage cutting-edge technology and offer a\n",
      "membership program with benefits for food and quick commerce dine-and-drop services. Swiggy aims to\n",
      "provide a superior consumer experience and has recently launched Swiggy Lite membership, which\n",
      "offers extra benefits and deletes consumer delivery charges. Additionally, Swiggy has launched an\n",
      "incognito mode for grocery orders and food.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# ğŸ¯ Final Output Handling\n",
    "# ============================\n",
    "try:\n",
    "    # Use regex to extract the JSON part from the string\n",
    "    json_match = re.search(r'```json\\n(.*?)\\n```', final_output, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_string = json_match.group(1)\n",
    "        parsed = json.loads(json_string)\n",
    "\n",
    "        # âœ… Improve keywords using KeyBERT\n",
    "        # Check if clean_text is available before using KeyBERT\n",
    "        if 'clean_text' in locals():\n",
    "            kw_model = KeyBERT(model=SentenceTransformer('all-MiniLM-L6-v2'))\n",
    "            kb_keywords = kw_model.extract_keywords(\n",
    "                clean_text,\n",
    "                keyphrase_ngram_range=(1, 2),\n",
    "                stop_words='english',\n",
    "                top_n=10,\n",
    "                use_maxsum=True,\n",
    "                nr_candidates=20\n",
    "            )\n",
    "            final_keywords = [kw for kw, score in kb_keywords]\n",
    "            parsed[\"keywords\"] = final_keywords\n",
    "        else:\n",
    "            print(\"Warning: 'clean_text' not available for keyword extraction using KeyBERT.\")\n",
    "\n",
    "\n",
    "        # âœ… Pretty output\n",
    "        print(\"\\nâœ… Final Metadata:\")\n",
    "        print(json.dumps(parsed, indent=2))\n",
    "\n",
    "        print(\"\\nâœ… Final Summary:\")\n",
    "        if \"summary\" in parsed and parsed[\"summary\"]:\n",
    "             print(textwrap.fill(parsed[\"summary\"], width=100))\n",
    "        else:\n",
    "            print(\"Summary not available in the parsed output.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"âš ï¸ Could not find the JSON object within the final output string.\")\n",
    "        print(\"Showing raw output:\")\n",
    "        print(final_output)\n",
    "\n",
    "except json.JSONDecodeError:\n",
    "    print(\"âš ï¸ Could not parse JSON. Showing raw output:\")\n",
    "    print(final_output)\n",
    "except KeyError as e:\n",
    "     print(f\"âš ï¸ KeyError: {e} - Check if expected keys are present in the JSON output.\")\n",
    "     print(\"Showing parsed dictionary (if available):\")\n",
    "     if 'parsed' in locals():\n",
    "         print(json.dumps(parsed, indent=2))\n",
    "     else:\n",
    "         print(\"Parsed dictionary not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1750416516568,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "NlSwOwrw5mxe"
   },
   "outputs": [],
   "source": [
    "# def call_llm_merge_summary(prompt):\n",
    "#     headers = {\n",
    "#         \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
    "#         \"Content-Type\": \"application/json\"\n",
    "#      }\n",
    "#     data = {\n",
    "#         \"model\": \"open-mistral-7b\",\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#         \"temperature\": 0.3\n",
    "#     }\n",
    "#     response = requests.post(os.getenv(\"MISTRAL_API_URL\"), headers=headers, json=data)\n",
    "#     return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "# final_output = call_llm_merge_summary(combine_prompt)\n",
    "# print(\"\\nâœ… Final Combined Metadata & Summary:\\n\")\n",
    "# print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1750416519106,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "4p8M47IYzowz"
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Let's say this is the response string\n",
    "# final_output = call_llm_merge_summary(combine_prompt)\n",
    "\n",
    "# try:\n",
    "#     parsed = json.loads(final_output)\n",
    "#     print(\"âœ… Summary:\\n\", parsed[\"summary\"])\n",
    "# except json.JSONDecodeError:\n",
    "#     print(\"âŒ Could not parse JSON. Showing raw output:\\n\")\n",
    "#     print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1750416521761,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "7W56o22N0cfF"
   },
   "outputs": [],
   "source": [
    "# import textwrap\n",
    "\n",
    "# summary_text = parsed[\"summary\"]\n",
    "# wrapped_summary = textwrap.fill(summary_text, width=100)\n",
    "# print(\"âœ… Final Summary:\\n\")\n",
    "# print(wrapped_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9549,
     "status": "ok",
     "timestamp": 1750416588717,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "ccYya9RE9Rhz",
    "outputId": "c60088d3-b5c7-47b1-d979-ce57bea73cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.46.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
      "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.43.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.46.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
      "Successfully installed pydeck-0.9.1 streamlit-1.46.0 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1750416592261,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "hCjiT9qe9I0u"
   },
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1750416869374,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "aaTCNFdf9ETL",
    "outputId": "59a6c62d-feef-4b30-d93a-86885b94b96d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 10:54:28.951 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:54:29.176 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
      "2025-06-20 10:54:29.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:54:29.178 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:54:29.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:54:29.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:54:29.181 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:54:29.183 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:54:29.184 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 10:54:29.186 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Streamlit UI\n",
    "st.title(\"ğŸ“„ Auto Metadata & Summary Generator\")\n",
    "file = st.file_uploader(\"Upload PDF, DOCX or TXT file\", type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "if file:\n",
    "    with st.spinner(\"Extracting and processing file...\"):\n",
    "        raw_text = extract_text_from_file(file)\n",
    "        clean_text = preprocess_text(raw_text)\n",
    "        chunks = split_text_into_chunks(clean_text)\n",
    "        summaries = [call_mistral(PROMPT_TEMPLATE.format(content_chunk=chunk)) for chunk in chunks]\n",
    "        combined = \"\\n\\n\".join(summaries)\n",
    "\n",
    "        # Final summary generation\n",
    "        final_prompt = f\"\"\"\n",
    "You are a smart assistant. Below are multiple partial summaries of a document, generated from different parts.\n",
    "Your task is to combine them into a **single metadata + summary JSON**, like this:\n",
    "{{\n",
    "  \"title\": \"\",\n",
    "  \"author\": \"\",\n",
    "  \"date\": \"\",\n",
    "  \"keywords\": [],\n",
    "  \"document_type\": \"\",\n",
    "  \"summary\": \"\"\n",
    "}}\n",
    "\n",
    "Summaries:\n",
    "{combined}\n",
    "\"\"\"\n",
    "        final_output = call_mistral(final_prompt)\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(final_output)\n",
    "        kb_keywords = kw_model.extract_keywords(\n",
    "            clean_text,\n",
    "            keyphrase_ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            top_n=10,\n",
    "            use_maxsum=True,\n",
    "            nr_candidates=20\n",
    "        )\n",
    "        final_keywords = [kw for kw, _ in kb_keywords]\n",
    "        parsed['keywords'] = final_keywords\n",
    "\n",
    "        st.subheader(\"ğŸ“Œ Extracted Metadata\")\n",
    "        st.json(parsed)\n",
    "\n",
    "        st.subheader(\"ğŸ“ Wrapped Summary\")\n",
    "        st.text(textwrap.fill(parsed[\"summary\"], width=100))\n",
    "\n",
    "        # Download summary\n",
    "        st.download_button(\n",
    "            label=\"ğŸ’¾ Download Summary\",\n",
    "            data=parsed[\"summary\"],\n",
    "            file_name=\"summary.txt\",\n",
    "            mime=\"text/plain\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to parse output: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7899,
     "status": "ok",
     "timestamp": 1750417126224,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "Vp13Reh5_Ydt",
    "outputId": "2a15512b-c50f-451f-bff8-f636b3a0b824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.2.11\n"
     ]
    }
   ],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1750417208809,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "hZuO-szO_qk5"
   },
   "outputs": [],
   "source": [
    "code = '''\n",
    "# Streamlit UI\n",
    "st.title(\"ğŸ“„ Auto Metadata & Summary Generator\")\n",
    "file = st.file_uploader(\"Upload PDF, DOCX or TXT file\", type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "if file:\n",
    "    with st.spinner(\"Extracting and processing file...\"):\n",
    "        raw_text = extract_text_from_file(file)\n",
    "        clean_text = preprocess_text(raw_text)\n",
    "        chunks = split_text_into_chunks(clean_text)\n",
    "        summaries = [call_mistral(PROMPT_TEMPLATE.format(content_chunk=chunk)) for chunk in chunks]\n",
    "        combined = \"\\n\\n\".join(summaries)\n",
    "\n",
    "        # Final summary generation\n",
    "        final_prompt = f\"\"\"\n",
    "You are a smart assistant. Below are multiple partial summaries of a document, generated from different parts.\n",
    "Your task is to combine them into a **single metadata + summary JSON**, like this:\n",
    "{{\n",
    "  \"title\": \"\",\n",
    "  \"author\": \"\",\n",
    "  \"date\": \"\",\n",
    "  \"keywords\": [],\n",
    "  \"document_type\": \"\",\n",
    "  \"summary\": \"\"\n",
    "}}\n",
    "\n",
    "Summaries:\n",
    "{combined}\n",
    "\"\"\"\n",
    "        final_output = call_mistral(final_prompt)\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(final_output)\n",
    "        kb_keywords = kw_model.extract_keywords(\n",
    "            clean_text,\n",
    "            keyphrase_ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            top_n=10,\n",
    "            use_maxsum=True,\n",
    "            nr_candidates=20\n",
    "        )\n",
    "        final_keywords = [kw for kw, _ in kb_keywords]\n",
    "        parsed['keywords'] = final_keywords\n",
    "\n",
    "        st.subheader(\"ğŸ“Œ Extracted Metadata\")\n",
    "        st.json(parsed)\n",
    "\n",
    "        st.subheader(\"ğŸ“ Wrapped Summary\")\n",
    "        st.text(textwrap.fill(parsed[\"summary\"], width=100))\n",
    "\n",
    "        # Download summary\n",
    "        st.download_button(\n",
    "            label=\"ğŸ’¾ Download Summary\",\n",
    "            data=parsed[\"summary\"],\n",
    "            file_name=\"summary.txt\",\n",
    "            mime=\"text/plain\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to parse output: {e}\")\n",
    "'''\n",
    "with open(\"streamlit_metadata_app.py\", \"w\") as f:\n",
    "    f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "error",
     "timestamp": 1750417507637,
     "user": {
      "displayName": "Akshat Jain",
      "userId": "06113647843579082661"
     },
     "user_tz": -330
    },
    "id": "BWLHxH5-_0WZ",
    "outputId": "76130f2f-301f-4baf-c57b-3d338ba1452b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyngrok.process.ngrok:t=2025-06-20T11:05:07+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
     ]
    },
    {
     "ename": "PyngrokNgrokError",
     "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-119-1804560191.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Expose the Streamlit app running on port 8501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ”— Click here to open your app: {public_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
      "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBSFGAvSHLdCD9Gx8sfslX",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
